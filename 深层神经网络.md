# 深层神经网络

> 这一章节的内容与上一章节其实差不多，尚未理解的是：多样例向量化后，反向传播中部分公式的推导。

## 深层神经网络中的前向与反向传播

> 多样例向量化后其公式会发生对应的变化，公式的变更更多的是由于引入了矩阵间的计算，导致理解偏差。

### 前向传播

> 尚未多样例向量化

**输入**：$a^{[l−1]}$

**输出**：$a^{[l]} , cache(z^{[l]})$

**公式**：
$$
Z^{[l]}=W^{[l]}\cdot a^{[l-1]}+b^{[l]}
$$ 
$$
a^{[l]}=g^{[l]}(Z^{[l]})
$$

### 反向传播

> 尚未多样里向量化

**输入**：$da^{[l]}$

**输出**：$da^{[l-1]} , dW^{[l]} , db^{[l]}$

**公式**：
$$
dZ^{[l]}=da^{[l]}*g^{[l]}{'}(Z^{[l]})
$$ 
$$
dW^{[l]}=dZ^{[l]}\cdot a^{[l-1]}
$$ 
$$
db^{[l]}=dZ^{[l]}
$$ 
$$
da^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}
$$

## 搭建深层神经网络块

![](./pictures/forward-and-backward-functions.png)

神经网络中一次训练包含完整的正向传播与反向传播过程。

## 矩阵的维度

$$
W^{[l]}: (n^{[l]}, n^{[l-1]})
$$ 
$$
b^{[l]}: (n^{[l]}, 1)
$$ 
$$
dW^{[l]}: (n^{[l]}, n^{[l-1]})
$$ 
$$
db^{[l]}: (n^{[l]}, 1)
$$

对于 $Z , a$ 

- 向量化前：

$$
Z^{[l]}, a^{[l]}: (n^{[l]}, 1)
$$

- 向量化后

$$
Z^{[l]}, A^{[l]}: (n^{[l]}, m)
$$

计算反向传播过程中，$dZ ,dA 的维度和Z ， A维度一致$

## 参数与超参数

**参数**即是我们在过程中想要模型学习到的信息（**模型自己能计算出来的**），例如 $ W^{[l]} , b^{[l]} $。。而**超参数（hyper parameters）**即为控制参数的输出值的一些网络信息（**需要人经验判断**）。超参数的改变会导致最终得到的参数 $W^{[l]} , b^{[l]}$。

典型的超参数有：

- 学习速率：α
- 迭代次数：N
- 隐藏层的层数：L
- 每一层的神经元个数：$n^{[1]} , n^{[2]} , \dots$

- 激活函数 g(z) 的选择

当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。