# 机器学习策略（1）

> 如何分析当前机器学习策略的表现，以及针对表现如何去改善等

对于一个已经被构建好且产生初步结果的机器学习系统，为了能使结果更令人满意，往往还要进行大量的改进。鉴于之前的课程介绍了多种改进的方法，例如收集更多数据、调试超参数、调整神经网络的大小或结构、采用不同的优化算法、进行正则化等等，我们有可能浪费大量时间在一条错误的改进路线上。

想要找准改进的方向，使一个机器学习系统更快更有效地工作，就需要学习一些在构建机器学习系统时常用到的策略。

## 正交化

> 正交化我这里认为是一种思考问题的方式，将问题正交化，针对某问题的调试并不会影响其他问题的表现。

**正交化（Orthogonalization）**的核心在于每次调整只会影响模型某一方面的性能，而对其他功能没有影响。这种方法有助于更快更有效地进行机器学习模型的调试和优化。

在机器学习（监督学习）系统中，可以划分四个“功能”：

1. 建立的模型在训练集上表现良好；
2. 建立的模型在验证集上表现良好；
3. 建立的模型在测试集上表现良好；
4. 建立的模型在实际应用中表现良好。

其中，

- 对于第一条，如果模型在训练集上表现不好，可以尝试训练更大的神经网络或者换一种更好的优化算法（例如 Adam）；
- 对于第二条，如果模型在验证集上表现不好，可以进行正则化处理或者加入更多训练数据；
- 对于第三条，如果模型在测试集上表现不好，可以尝试使用更大的验证集进行验证；
- 对于第四条，如果模型在实际应用中表现不好，可能是因为测试集没有设置正确或者成本函数评估指标有误，需要改变测试集或成本函数。

面对遇到的各种问题，正交化能够帮助我们更为精准有效地解决问题。

一个反例是[早停止法（Early Stopping）](http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/深度学习的实用层面?id=其他正则化方法)。如果早期停止，虽然可以改善验证集的拟合表现，但是对训练集的拟合就不太好。因为对两个不同的“功能”都有影响，所以早停止法不具有正交化。虽然也可以使用，但是用其他正交化控制手段来进行优化会更简单有效。

### 单值评价指标

构建机器学习系统时，通过设置一个量化的**单值评价指标**（single-number evaluation metric），可以使我们根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。单值评价指标是通过综合衡量多个指标的情况下，找到的单个衡量指标，用于有效的判断当前模型的优劣。

例如，对于二分类问题，常用的评价指标是**精确率（Precision）**和**召回率（Recall）**。假设我们有 A 和 B 两个分类器，其两项指标分别如下：

| 分类器 | 准确率 | 召回率 |
| ------ | ------ | ------ |
| A      | 95%    | 90%    |
| B      | 98%    | 85%    |

- 精确率：$\frac{预测为正类的正类数量}{预测为正类的数量} * 100%$
- 召回率：$\frac{预测为正类的正类数量}{正类数量} * 100%$

实际应用中，我们通常使用综合了精确率和召回率的单值评价指标 F1 Score 来评价模型的好坏。F1 Score 其实就是精准率和召回率的**调和平均数（Harmonic Mean）**，比单纯的平均数效果要好。
$$
F1 = \frac{2}{\frac{1}{P}+\frac{1}{R}} = \frac{2PR}{P+R}
$$
因此，我们计算出两个分类器的 F1 Score。可以看出 A 模型的效果要更好。

| 分类器 | 准确率 | 召回率 | F1 Score |
| ------ | ------ | ------ | -------- |
| A      | 95%    | 90%    | 92.4%    |
| B      | 98%    | 85%    | 91%      |

通过引入单值评价指标，我们可以更方便快速地对不同模型进行比较。

### 优化指标与满足指标

如果我们还想要将分类器的运行时间也纳入考虑范围，将其和精确率、召回率组合成一个单值评价指标显然不那么合适。这时，我们可以将某些指标作为**优化指标（Optimizing Matric）**，寻求它们的最优值；而将某些指标作为**满足指标（Satisficing Matric）**，只要在一定阈值以内即可。

优化指标与满足指标在直觉上有主要与次要的差别，优化指标是无论如何都要好的，满足指标只要满足阈值即可。

在这个例子中，准确率就是一个优化指标，因为我们想要分类器尽可能做到正确分类；而运行时间就是一个满足指标，如果你想要分类器的运行时间不多于某个阈值，那最终选择的分类器就应该是以这个阈值为界里面准确率最高的那个。

## 比较人类表现水平

很多机器学习模型的诞生是为了取代人类的工作，因此其表现也会跟人类表现水平作比较。

![](./pictures/Bayes-Optimal-Error.png)

上图展示了随着时间的推进，机器学习系统和人的表现水平的变化。一般的，当机器学习超过人的表现水平后，它的进步速度逐渐变得缓慢，最终性能无法超过某个理论上限，这个上限被称为**贝叶斯最优误差（Bayes Optimal Error）**。

贝叶斯最优误差一般认为是理论上可能达到的最优误差，换句话说，其就是理论最优函数，任何从 x 到精确度 y 映射的函数都不可能超过这个值。例如，对于语音识别，某些音频片段嘈杂到基本不可能知道说的是什么，所以完美的识别率不可能达到 100%。

因为人类对于一些自然感知问题的表现水平十分接近贝叶斯最优误差，所以当机器学习系统的表现超过人类后，就没有太多继续改善的空间了。

也因此，只要建立的机器学习模型的表现还没达到人类的表现水平时，就可以通过各种手段来提升它。例如采用人工标记过的数据进行训练，通过人工误差分析了解为什么人能够正确识别，或者是进行偏差、方差分析。

当模型的表现超过人类后，这些手段起的作用就微乎其微了。

### 可避免偏差与可避免方差

通过与贝叶斯最优误差，或者说，与人类表现水平的比较，可以表明一个机器学习模型表现的好坏程度，由此判断后续操作应该注重于减小偏差还是减小方差。

- 可避免偏差：**人类水平**误差与**训练集**上模型的误差的差值；
- 可避免方差：**训练集**误差与**验证集**上模型的误差的差值；

模型在**训练集**上的误差与人类表现水平的差值被称作**可避免偏差（Avoidable Bias）**。可避免偏差低便意味着模型在训练集上的表现很好，而**训练集与验证集之间错误率的差值**越小，意味着模型在验证集与测试集上的表现和训练集同样好。

如果**可避免偏差**大于**训练集与验证集之间错误率的差值**，之后的工作就应该专注于减小偏差；反之，就应该专注于减小方差。

### 理解人类表现水平

我们一般用**人类水平误差（Human-level Error）**来代表贝叶斯最优误差（或者简称贝叶斯误差）。对于不同领域的例子，不同人群由于其经验水平不一，错误率也不同。一般来说，**我们将表现最好的作为人类水平误差**。但是实际应用中，不同人选择人类水平误差的基准是不同的，这会带来一定的影响。

例如，如果某模型在训练集上的错误率为 0.7%，验证集的错误率为 0.8%。如果选择的人类水平误差为 0.5%，那么偏差（bias）比方差（variance）更加突出；而如果选择的人类水平误差为 0.7%，则方差更加突出。也就是说，根据人类水平误差的不同选择，我们可能因此选择不同的优化操作。

这种问题只会发生在模型表现很好，接近人类水平误差的时候才会出现。人类水平误差给了我们一种估计贝叶斯误差的方式，而不是像之前一样将训练的错误率直接对着 0% 的方向进行优化。

当机器学习模型的表现超过了人类水平误差时，很难再通过人的直觉去判断模型还能够往什么方向优化以提高性能。

## 总结

想让一个监督学习算法达到使用程度，应该做到以下两点：

1. 算法对训练集的拟合很好，可以看作可避免偏差很低；
2. 推广到验证集和测试集效果也很好，即方差不是很大。

![](./pictures/Human-level.png)

